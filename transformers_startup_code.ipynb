{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "RhHpuCvsI6V-"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "import numpy as np\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Prepare data\n",
    "\n",
    "class VocabEntry(object):\n",
    "    \"\"\"docstring for Vocab\"\"\"\n",
    "    def __init__(self, word2id=None):\n",
    "        super(VocabEntry, self).__init__()\n",
    "\n",
    "        if word2id:\n",
    "            self.word2id = word2id\n",
    "            self.unk_id = word2id['<unk>']\n",
    "        else:\n",
    "            self.word2id = dict()\n",
    "            self.unk_id = 3\n",
    "            self.word2id['<pad>'] = 0\n",
    "            self.word2id['<s>'] = 1\n",
    "            self.word2id['</s>'] = 2\n",
    "            self.word2id['<unk>'] = self.unk_id\n",
    "\n",
    "        self.id2word_ = {v: k for k, v in self.word2id.items()}\n",
    "\n",
    "    def __getitem__(self, word):\n",
    "        return self.word2id.get(word, self.unk_id)\n",
    "\n",
    "    def __contains__(self, word):\n",
    "        return word in self.word2id\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.word2id)\n",
    "\n",
    "    def add(self, word):\n",
    "        if word not in self:\n",
    "            wid = self.word2id[word] = len(self)\n",
    "            self.id2word[wid] = word\n",
    "            return wid\n",
    "\n",
    "        else:\n",
    "            return self[word]\n",
    "\n",
    "    def id2word(self, wid):\n",
    "        return self.id2word_[wid]\n",
    "\n",
    "    def decode_sentence(self, sentence):\n",
    "        decoded_sentence = []\n",
    "        for wid_t in sentence:\n",
    "            wid = wid_t.item()\n",
    "            decoded_sentence.append(self.id2word_[wid])\n",
    "        return decoded_sentence\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def from_corpus(fname):\n",
    "        vocab = VocabEntry()\n",
    "        with open(fname) as fin:\n",
    "            for line in fin:\n",
    "                _ = [vocab.add(word) for word in line.split()]\n",
    "\n",
    "        return vocab\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MonoTextData(object):\n",
    "    \"\"\"docstring for MonoTextData\"\"\"\n",
    "    def __init__(self, fname, label=False, max_length=None, vocab=None):\n",
    "        super(MonoTextData, self).__init__()\n",
    "\n",
    "        self.data, self.vocab, self.dropped, self.labels = self._read_corpus(fname, label, max_length, vocab)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def _read_corpus(self, fname, label, max_length, vocab):\n",
    "        data = []\n",
    "        labels = [] if label else None\n",
    "        dropped = 0\n",
    "        if not vocab:\n",
    "            vocab = defaultdict(lambda: len(vocab))\n",
    "            vocab['<pad>'] = 0\n",
    "            vocab['<s>'] = 1\n",
    "            vocab['</s>'] = 2\n",
    "            vocab['<unk>'] = 3\n",
    "\n",
    "        with open(fname) as fin:\n",
    "            for line in fin:\n",
    "                if label:\n",
    "                    split_line = line.split('\\t')\n",
    "                    lb = split_line[0]\n",
    "                    split_line = split_line[1].split()\n",
    "                else:\n",
    "                    split_line = line.split('\\t')[0]\n",
    "                if len(split_line) < 1:\n",
    "                    dropped += 1\n",
    "                    continue\n",
    "\n",
    "                if max_length:\n",
    "                    if len(split_line) > max_length:\n",
    "                        dropped += 1\n",
    "                        continue\n",
    "\n",
    "                if label:\n",
    "                    labels.append(lb)\n",
    "                data.append([vocab[word] for word in split_line])\n",
    "\n",
    "        if isinstance(vocab, VocabEntry):\n",
    "            return data, vocab, dropped, labels\n",
    "\n",
    "        return data, VocabEntry(vocab), dropped, labels\n",
    "\n",
    "    def _to_tensor(self, batch_data, batch_first, device):\n",
    "        \"\"\"pad a list of sequences, and transform them to tensors\n",
    "        Args:\n",
    "            batch_data: a batch of sentences (list) that are composed of\n",
    "                word ids.\n",
    "            batch_first: If true, the returned tensor shape is\n",
    "                (batch, seq_len), otherwise (seq_len, batch)\n",
    "            device: torch.device\n",
    "        Returns: Tensor, Int list\n",
    "            Tensor: Tensor of the batch data after padding\n",
    "            Int list: a list of integers representing the length\n",
    "                of each sentence (including start and stop symbols)\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "        # pad stop symbol\n",
    "        batch_data = [sent + [self.vocab['</s>']] for sent in batch_data]\n",
    "\n",
    "        sents_len = [len(sent) for sent in batch_data]\n",
    "\n",
    "        max_len = max(sents_len)\n",
    "\n",
    "        batch_size = len(sents_len)\n",
    "        sents_new = []\n",
    "\n",
    "        # pad start symbol\n",
    "        sents_new.append([self.vocab['<s>']] * batch_size)\n",
    "        for i in range(max_len):\n",
    "            sents_new.append([sent[i] if len(sent) > i else self.vocab['<pad>'] \\\n",
    "                               for sent in batch_data])\n",
    "\n",
    "\n",
    "        sents_ts = torch.tensor(sents_new, dtype=torch.long,\n",
    "                                 requires_grad=False, device=device)\n",
    "\n",
    "        if batch_first:\n",
    "            sents_ts = sents_ts.permute(1, 0).contiguous()\n",
    "\n",
    "        return sents_ts, [length + 1 for length in sents_len]\n",
    "\n",
    "\n",
    "    def data_iter(self, batch_size, device, batch_first=False, shuffle=True):\n",
    "        \"\"\"pad data with start and stop symbol, and pad to the same length\n",
    "        Returns:\n",
    "            batch_data: LongTensor with shape (seq_len, batch_size)\n",
    "            sents_len: list of data length, this is the data length\n",
    "                       after counting start and stop symbols\n",
    "        \"\"\"\n",
    "        index_arr = np.arange(len(self.data))\n",
    "\n",
    "        if shuffle:\n",
    "            np.random.shuffle(index_arr)\n",
    "\n",
    "        batch_num = int(np.ceil(len(index_arr)) / float(batch_size))\n",
    "        for i in range(batch_num):\n",
    "            batch_ids = index_arr[i * batch_size : (i+1) * batch_size]\n",
    "            batch_data = [self.data[index] for index in batch_ids]\n",
    "\n",
    "            # uncomment this line if the dataset has variable length\n",
    "            batch_data.sort(key=lambda e: -len(e))\n",
    "\n",
    "            batch_data, sents_len = self._to_tensor(batch_data, batch_first, device)\n",
    "\n",
    "            yield batch_data, sents_len\n",
    "\n",
    "    \n",
    "\n",
    "    def create_data_batch(self, batch_size, device, batch_first=False):\n",
    "        \"\"\"pad data with start and stop symbol, batching is performerd w.r.t.\n",
    "        the sentence length, so that each returned batch has the same length,\n",
    "        no further pack sequence function (e.g. pad_packed_sequence) is required\n",
    "        Returns: List\n",
    "            List: a list of batched data, each element is a tensor with shape\n",
    "                (seq_len, batch_size)\n",
    "        \"\"\"\n",
    "        sents_len = np.array([len(sent) for sent in self.data])\n",
    "        sort_idx = np.argsort(sents_len)\n",
    "        sort_len = sents_len[sort_idx]\n",
    "\n",
    "        # record the locations where length changes\n",
    "        change_loc = []\n",
    "        for i in range(1, len(sort_len)):\n",
    "            if sort_len[i] != sort_len[i-1]:\n",
    "                change_loc.append(i)\n",
    "        change_loc.append(len(sort_len))\n",
    "\n",
    "        batch_data_list = []\n",
    "        total = 0\n",
    "        curr = 0\n",
    "        for idx in change_loc:\n",
    "            while curr < idx:\n",
    "                batch_data = []\n",
    "                next = min(curr + batch_size, idx)\n",
    "                for id_ in range(curr, next):\n",
    "                    batch_data.append(self.data[sort_idx[id_]])\n",
    "                curr = next\n",
    "                batch_data, sents_len = self._to_tensor(batch_data, batch_first, device)\n",
    "                batch_data_list.append(batch_data)\n",
    "\n",
    "                total += batch_data.size(0)\n",
    "                assert(sents_len == ([sents_len[0]] * len(sents_len)))\n",
    "\n",
    "        assert(total == len(self.data))\n",
    "        return batch_data_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "VArxuOMqKLjw"
   },
   "outputs": [],
   "source": [
    "train_data = MonoTextData('dataset/turkish-train-low.txt')\n",
    "vocab = train_data.vocab\n",
    "#val_data = MonoTextData('turkish-dev', vocab=vocab)\n",
    "#tst_data = MonoTextData('turkish-test', vocab=vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8VbngezNJfS_",
    "outputId": "b0a5d501-8586-4e0d-de8a-dd5a04912601"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(<function __main__.MonoTextData._read_corpus.<locals>.<lambda>()>,\n",
       "            {'<pad>': 0,\n",
       "             '<s>': 1,\n",
       "             '</s>': 2,\n",
       "             '<unk>': 3,\n",
       "             'o': 4,\n",
       "             't': 5,\n",
       "             'u': 6,\n",
       "             'z': 7,\n",
       "             ' ': 8,\n",
       "             'b': 9,\n",
       "             'i': 10,\n",
       "             'r': 11,\n",
       "             'c': 12,\n",
       "             'h': 13,\n",
       "             'e': 14,\n",
       "             'y': 15,\n",
       "             'k': 16,\n",
       "             'ö': 17,\n",
       "             'ü': 18,\n",
       "             'l': 19,\n",
       "             'ş': 20,\n",
       "             'm': 21,\n",
       "             'a': 22,\n",
       "             'n': 23,\n",
       "             'ı': 24,\n",
       "             'd': 25,\n",
       "             's': 26,\n",
       "             'ğ': 27,\n",
       "             'p': 28,\n",
       "             'ç': 29,\n",
       "             'v': 30,\n",
       "             'g': 31,\n",
       "             'f': 32,\n",
       "             'L': 33,\n",
       "             'j': 34})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab.word2id # character vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "SI_O9HEsJhYQ"
   },
   "outputs": [],
   "source": [
    "train_data_batch = train_data.create_data_batch(batch_size=32, device='cuda', batch_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IM1lfwgsK1Xu"
   },
   "outputs": [],
   "source": [
    "## Create model (with transformers)\n",
    "model = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r7NRaLbdJk5B"
   },
   "outputs": [],
   "source": [
    "## Train\n",
    "epochs = 50\n",
    "for epoch in range(epochs):\n",
    "    loss = 0\n",
    "    epoch_acc = 0\n",
    "    epoch_tokens = 0  \n",
    "    for i in np.random.permutation(len(train_data_batch)):\n",
    "        batch_data = train_data_batch[i]\n",
    "        batch_size, word_len = batch_data.size()\n",
    "        # calculate model loss with batch_data and update gradients"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "transformers-startup-code.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
