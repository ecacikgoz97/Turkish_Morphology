{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "RhHpuCvsI6V-"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "import numpy as np\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Prepare data\n",
    "\n",
    "class VocabEntry(object):\n",
    "    \"\"\"docstring for Vocab\"\"\"\n",
    "    def __init__(self, word2id=None):\n",
    "        super(VocabEntry, self).__init__()\n",
    "\n",
    "        if word2id:\n",
    "            self.word2id = word2id\n",
    "            self.unk_id = word2id['<unk>']\n",
    "        else:\n",
    "            self.word2id = dict()\n",
    "            self.unk_id = 3\n",
    "            self.word2id['<pad>'] = 0\n",
    "            self.word2id['<s>'] = 1\n",
    "            self.word2id['</s>'] = 2\n",
    "            self.word2id['<unk>'] = self.unk_id\n",
    "\n",
    "        self.id2word_ = {v: k for k, v in self.word2id.items()}\n",
    "\n",
    "    def __getitem__(self, word):\n",
    "        return self.word2id.get(word, self.unk_id)\n",
    "\n",
    "    def __contains__(self, word):\n",
    "        return word in self.word2id\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.word2id)\n",
    "\n",
    "    def add(self, word):\n",
    "        if word not in self:\n",
    "            wid = self.word2id[word] = len(self)\n",
    "            self.id2word[wid] = word\n",
    "            return wid\n",
    "\n",
    "        else:\n",
    "            return self[word]\n",
    "\n",
    "    def id2word(self, wid):\n",
    "        return self.id2word_[wid]\n",
    "\n",
    "    def decode_sentence(self, sentence):\n",
    "        decoded_sentence = []\n",
    "        for wid_t in sentence:\n",
    "            wid = wid_t.item()\n",
    "            decoded_sentence.append(self.id2word_[wid])\n",
    "        return decoded_sentence\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def from_corpus(fname):\n",
    "        vocab = VocabEntry()\n",
    "        with open(fname) as fin:\n",
    "            for line in fin:\n",
    "                _ = [vocab.add(word) for word in line.split()]\n",
    "\n",
    "        return vocab\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MonoTextData(object):\n",
    "    \"\"\"docstring for MonoTextData\"\"\"\n",
    "    def __init__(self, fname, label=False, max_length=None, vocab=None):\n",
    "        super(MonoTextData, self).__init__()\n",
    "\n",
    "        self.data, self.vocab, self.dropped, self.labels = self._read_corpus(fname, label, max_length, vocab)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def _read_corpus(self, fname, label, max_length, vocab):\n",
    "        data = []\n",
    "        labels = [] if label else None\n",
    "        dropped = 0\n",
    "        if not vocab:\n",
    "            vocab = defaultdict(lambda: len(vocab))\n",
    "            vocab['<pad>'] = 0\n",
    "            vocab['<s>'] = 1\n",
    "            vocab['</s>'] = 2\n",
    "            vocab['<unk>'] = 3\n",
    "\n",
    "        with open(fname) as fin:\n",
    "            for line in fin:\n",
    "                if label:\n",
    "                    split_line = line.split('\\t')\n",
    "                    lb = split_line[0]\n",
    "                    split_line = split_line[1].split()\n",
    "                else:\n",
    "                    split_line = line.split('\\t')[0]\n",
    "                if len(split_line) < 1:\n",
    "                    dropped += 1\n",
    "                    continue\n",
    "\n",
    "                if max_length:\n",
    "                    if len(split_line) > max_length:\n",
    "                        dropped += 1\n",
    "                        continue\n",
    "\n",
    "                if label:\n",
    "                    labels.append(lb)\n",
    "                data.append([vocab[word] for word in split_line])\n",
    "\n",
    "        if isinstance(vocab, VocabEntry):\n",
    "            return data, vocab, dropped, labels\n",
    "\n",
    "        return data, VocabEntry(vocab), dropped, labels\n",
    "\n",
    "    def _to_tensor(self, batch_data, batch_first, device):\n",
    "        \"\"\"pad a list of sequences, and transform them to tensors\n",
    "        Args:\n",
    "            batch_data: a batch of sentences (list) that are composed of\n",
    "                word ids.\n",
    "            batch_first: If true, the returned tensor shape is\n",
    "                (batch, seq_len), otherwise (seq_len, batch)\n",
    "            device: torch.device\n",
    "        Returns: Tensor, Int list\n",
    "            Tensor: Tensor of the batch data after padding\n",
    "            Int list: a list of integers representing the length\n",
    "                of each sentence (including start and stop symbols)\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "        # pad stop symbol\n",
    "        batch_data = [sent + [self.vocab['</s>']] for sent in batch_data]\n",
    "\n",
    "        sents_len = [len(sent) for sent in batch_data]\n",
    "\n",
    "        max_len = max(sents_len)\n",
    "\n",
    "        batch_size = len(sents_len)\n",
    "        sents_new = []\n",
    "\n",
    "        # pad start symbol\n",
    "        sents_new.append([self.vocab['<s>']] * batch_size)\n",
    "        for i in range(max_len):\n",
    "            sents_new.append([sent[i] if len(sent) > i else self.vocab['<pad>'] \\\n",
    "                               for sent in batch_data])\n",
    "\n",
    "\n",
    "        sents_ts = torch.tensor(sents_new, dtype=torch.long,\n",
    "                                 requires_grad=False, device=device)\n",
    "\n",
    "        if batch_first:\n",
    "            sents_ts = sents_ts.permute(1, 0).contiguous()\n",
    "\n",
    "        return sents_ts, [length + 1 for length in sents_len]\n",
    "\n",
    "\n",
    "    def data_iter(self, batch_size, device, batch_first=False, shuffle=True):\n",
    "        \"\"\"pad data with start and stop symbol, and pad to the same length\n",
    "        Returns:\n",
    "            batch_data: LongTensor with shape (seq_len, batch_size)\n",
    "            sents_len: list of data length, this is the data length\n",
    "                       after counting start and stop symbols\n",
    "        \"\"\"\n",
    "        index_arr = np.arange(len(self.data))\n",
    "\n",
    "        if shuffle:\n",
    "            np.random.shuffle(index_arr)\n",
    "\n",
    "        batch_num = int(np.ceil(len(index_arr)) / float(batch_size))\n",
    "        for i in range(batch_num):\n",
    "            batch_ids = index_arr[i * batch_size : (i+1) * batch_size]\n",
    "            batch_data = [self.data[index] for index in batch_ids]\n",
    "\n",
    "            # uncomment this line if the dataset has variable length\n",
    "            batch_data.sort(key=lambda e: -len(e))\n",
    "\n",
    "            batch_data, sents_len = self._to_tensor(batch_data, batch_first, device)\n",
    "\n",
    "            yield batch_data, sents_len\n",
    "\n",
    "    \n",
    "\n",
    "    def create_data_batch(self, batch_size, device, batch_first=False):\n",
    "        \"\"\"pad data with start and stop symbol, batching is performerd w.r.t.\n",
    "        the sentence length, so that each returned batch has the same length,\n",
    "        no further pack sequence function (e.g. pad_packed_sequence) is required\n",
    "        Returns: List\n",
    "            List: a list of batched data, each element is a tensor with shape\n",
    "                (seq_len, batch_size)\n",
    "        \"\"\"\n",
    "        sents_len = np.array([len(sent) for sent in self.data])\n",
    "        sort_idx = np.argsort(sents_len)\n",
    "        sort_len = sents_len[sort_idx]\n",
    "\n",
    "        # record the locations where length changes\n",
    "        change_loc = []\n",
    "        for i in range(1, len(sort_len)):\n",
    "            if sort_len[i] != sort_len[i-1]:\n",
    "                change_loc.append(i)\n",
    "        change_loc.append(len(sort_len))\n",
    "\n",
    "        batch_data_list = []\n",
    "        total = 0\n",
    "        curr = 0\n",
    "        for idx in change_loc:\n",
    "            while curr < idx:\n",
    "                batch_data = []\n",
    "                next = min(curr + batch_size, idx)\n",
    "                for id_ in range(curr, next):\n",
    "                    batch_data.append(self.data[sort_idx[id_]])\n",
    "                curr = next\n",
    "                batch_data, sents_len = self._to_tensor(batch_data, batch_first, device)\n",
    "                batch_data_list.append(batch_data)\n",
    "\n",
    "                total += batch_data.size(0)\n",
    "                assert(sents_len == ([sents_len[0]] * len(sents_len)))\n",
    "\n",
    "        assert(total == len(self.data))\n",
    "        return batch_data_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "VArxuOMqKLjw"
   },
   "outputs": [],
   "source": [
    "train_data = MonoTextData('dataset/turkish-train-low.txt')\n",
    "vocab = train_data.vocab\n",
    "#val_data = MonoTextData('turkish-dev', vocab=vocab)\n",
    "#tst_data = MonoTextData('turkish-test', vocab=vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8VbngezNJfS_",
    "outputId": "b0a5d501-8586-4e0d-de8a-dd5a04912601"
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'collections.defaultdict' object has no attribute 'id2word'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-15bd64245950>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword2id\u001b[0m \u001b[0;31m# character vocab\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtoken\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid2word\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'collections.defaultdict' object has no attribute 'id2word'"
     ]
    }
   ],
   "source": [
    "token = vocab.word2id # character vocab\n",
    "token.id2word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'<pad>': 0,\n",
       " '<s>': 1,\n",
       " '</s>': 2,\n",
       " '<unk>': 3,\n",
       " 'o': 4,\n",
       " 't': 5,\n",
       " 'u': 6,\n",
       " 'z': 7,\n",
       " ' ': 8,\n",
       " 'b': 9,\n",
       " 'i': 10,\n",
       " 'r': 11,\n",
       " 'c': 12,\n",
       " 'h': 13,\n",
       " 'e': 14,\n",
       " 'y': 15,\n",
       " 'k': 16,\n",
       " 'ö': 17,\n",
       " 'ü': 18,\n",
       " 'l': 19,\n",
       " 'ş': 20,\n",
       " 'm': 21,\n",
       " 'a': 22,\n",
       " 'n': 23,\n",
       " 'ı': 24,\n",
       " 'd': 25,\n",
       " 's': 26,\n",
       " 'ğ': 27,\n",
       " 'p': 28,\n",
       " 'ç': 29,\n",
       " 'v': 30,\n",
       " 'g': 31,\n",
       " 'f': 32,\n",
       " 'L': 33,\n",
       " 'j': 34}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "SI_O9HEsJhYQ"
   },
   "outputs": [],
   "source": [
    "train_data_batch = train_data.create_data_batch(batch_size=32, device='cuda', batch_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IM1lfwgsK1Xu"
   },
   "outputs": [],
   "source": [
    "## Create model (with transformers)\n",
    "model = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "r7NRaLbdJk5B"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: batch_data: tensor([[ 1, 28, 14, 23, 10, 26,  2],\n",
      "        [ 1, 26, 22, 21,  6, 11,  2],\n",
      "        [ 1, 26,  4, 23, 11, 22,  2],\n",
      "        [ 1, 30, 22, 19, 10,  7,  2],\n",
      "        [ 1, 29, 24, 16,  5, 24,  2],\n",
      "        [ 1, 17, 28, 21, 14, 16,  2],\n",
      "        [ 1, 16, 17, 28, 18, 16,  2],\n",
      "        [ 1,  7, 10,  9, 10, 19,  2],\n",
      "        [ 1, 10, 29,  8, 22, 27,  2],\n",
      "        [ 1,  4,  5,  4, 12,  6,  2],\n",
      "        [ 1, 22, 19, 29, 22, 16,  2],\n",
      "        [ 1, 26,  4, 16, 22, 16,  2],\n",
      "        [ 1,  4, 16, 10, 15, 22,  2],\n",
      "        [ 1, 25, 22, 19, 22, 16,  2],\n",
      "        [ 1, 16, 14, 32, 22, 19,  2],\n",
      "        [ 1, 12, 10, 13, 22,  7,  2],\n",
      "        [ 1, 16, 14, 32, 22, 19,  2],\n",
      "        [ 1,  5,  4, 13,  6, 21,  2],\n",
      "        [ 1, 22, 19,  5, 24, 23,  2],\n",
      "        [ 1,  9, 22, 13, 29, 14,  2],\n",
      "        [ 1, 15, 22, 26, 22, 16,  2],\n",
      "        [ 1, 13, 14, 15, 14,  5,  2],\n",
      "        [ 1, 16, 22,  9,  6, 16,  2]], device='cuda:0'), batch_size: 23, word_len: 7\n",
      "1: batch_data: tensor([[ 1, 12, 22, 23, 19, 22, 23, 25, 24, 11,  5, 21, 22, 16,  2],\n",
      "        [ 1, 16, 24, 11, 21, 24,  7, 24,  8, 20, 22, 11, 22, 28,  2],\n",
      "        [ 1, 29, 10,  7, 31, 10, 19, 10,  8, 26, 10, 23, 14, 16,  2],\n",
      "        [ 1,  4, 19, 21, 22,  7, 26, 22,  8,  4, 19, 21, 22,  7,  2],\n",
      "        [ 1, 16, 17,  5, 18, 19, 14, 20,  5, 10, 11, 21, 14, 16,  2]],\n",
      "       device='cuda:0'), batch_size: 5, word_len: 15\n",
      "2: batch_data: tensor([[ 1,  9, 14, 19,  2],\n",
      "        [ 1, 14, 12, 14,  2]], device='cuda:0'), batch_size: 2, word_len: 5\n",
      "3: batch_data: tensor([[ 1, 29, 10,  7, 21, 14, 12, 10,  2],\n",
      "        [ 1, 17, 27, 11, 14, 23, 12, 10,  2],\n",
      "        [ 1, 15,  4, 23,  5, 21, 22, 16,  2],\n",
      "        [ 1, 16, 14, 25, 10, 12, 10, 16,  2],\n",
      "        [ 1,  5, 22, 23, 24, 21, 22, 16,  2],\n",
      "        [ 1, 21,  4,  9, 10, 19, 15, 22,  2],\n",
      "        [ 1, 26, 14, 30, 31, 10, 19, 10,  2],\n",
      "        [ 1,  5, 22, 30, 14, 11, 23, 22,  2],\n",
      "        [ 1, 17, 11, 18, 21, 12, 14, 16,  2],\n",
      "        [ 1, 28, 22,  5, 22,  5, 14, 26,  2],\n",
      "        [ 1, 22, 30,  4, 16, 22, 25,  4,  2],\n",
      "        [ 1, 22, 11, 24, 23, 21, 22, 16,  2]], device='cuda:0'), batch_size: 12, word_len: 9\n",
      "4: batch_data: tensor([[ 1,  4, 19, 22, 15,  8, 31, 18, 25, 18, 21, 19, 18,  8, 28, 11,  4, 31,\n",
      "         11, 22, 21, 19, 22, 21, 22,  2]], device='cuda:0'), batch_size: 1, word_len: 26\n",
      "5: batch_data: tensor([[ 1, 28, 22, 28, 22,  2],\n",
      "        [ 1,  4,  9, 34, 14,  2],\n",
      "        [ 1, 21, 14, 20, 14,  2],\n",
      "        [ 1,  4, 25, 22, 16,  2],\n",
      "        [ 1, 23, 14, 20, 14,  2],\n",
      "        [ 1, 16, 22, 11,  5,  2],\n",
      "        [ 1,  6,  7, 22, 15,  2],\n",
      "        [ 1, 11, 18, 15, 22,  2],\n",
      "        [ 1, 26,  4, 11,  6,  2],\n",
      "        [ 1, 16, 18, 28, 14,  2],\n",
      "        [ 1, 24, 20, 24, 23,  2],\n",
      "        [ 1, 14, 20, 15, 22,  2]], device='cuda:0'), batch_size: 12, word_len: 6\n",
      "6: batch_data: tensor([[ 1, 14, 23, 32, 19, 22, 26, 15,  4, 23,  2],\n",
      "        [ 1, 15, 14, 23, 25, 10, 11, 21, 14, 16,  2],\n",
      "        [ 1, 15, 14,  5, 14, 23, 14, 16, 19, 10,  2],\n",
      "        [ 1, 25,  6, 20,  8, 22, 19, 21, 22, 16,  2],\n",
      "        [ 1, 33, 14, 32, 16,  4, 20, 22, 19, 24,  2]], device='cuda:0'), batch_size: 5, word_len: 11\n",
      "7: batch_data: tensor([[ 1, 10,  5,  2],\n",
      "        [ 1, 10, 26,  2]], device='cuda:0'), batch_size: 2, word_len: 4\n",
      "8: batch_data: tensor([[ 1, 13, 14, 21,  7, 14, 21, 10, 23,  8, 31, 14, 29, 10,  5,  2]],\n",
      "       device='cuda:0'), batch_size: 1, word_len: 16\n",
      "9: batch_data: tensor([[ 1, 22,  5, 14, 20, 14,  8, 30, 14, 11, 21, 14, 16,  2],\n",
      "        [ 1, 22,  5, 14, 20, 14,  8, 30, 14, 11, 21, 14, 16,  2]],\n",
      "       device='cuda:0'), batch_size: 2, word_len: 14\n",
      "10: batch_data: tensor([[ 1,  4,  5,  6,  7,  8,  9, 10, 11, 12, 10,  2],\n",
      "        [ 1, 28, 22, 11, 22, 26, 24,  7, 19, 24, 16,  2]], device='cuda:0'), batch_size: 2, word_len: 12\n",
      "11: batch_data: tensor([[ 1, 26, 18, 11, 18, 23, 25, 18, 11, 21, 14, 16,  2],\n",
      "        [ 1, 17, 11, 18, 21, 12, 14, 16,  8, 22, 27, 24,  2],\n",
      "        [ 1, 15, 22, 11, 25, 24, 21, 12, 24, 19, 24, 16,  2],\n",
      "        [ 1, 14, 19,  8, 26, 24, 16, 24, 20, 21, 22, 16,  2],\n",
      "        [ 1, 26,  4, 11,  6, 21, 26,  6,  7, 19,  6, 16,  2]], device='cuda:0'), batch_size: 5, word_len: 13\n",
      "12: batch_data: tensor([[ 1, 28, 22, 28, 15,  4, 23,  2],\n",
      "        [ 1, 16,  4, 20,  6, 12,  6,  2],\n",
      "        [ 1, 28, 14, 29, 14,  5, 14,  2],\n",
      "        [ 1,  9,  4, 27, 21, 22, 16,  2],\n",
      "        [ 1, 25, 10, 11, 26, 14, 16,  2],\n",
      "        [ 1, 26, 14, 30, 21, 14, 16,  2],\n",
      "        [ 1, 16, 11, 22, 30, 22,  5,  2],\n",
      "        [ 1, 21, 22, 16, 22, 11, 22,  2],\n",
      "        [ 1, 26, 24, 27, 21, 22, 16,  2],\n",
      "        [ 1, 30, 14, 11, 23, 10, 16,  2],\n",
      "        [ 1, 26, 17, 23, 21, 14, 16,  2],\n",
      "        [ 1, 31, 14, 29, 21, 10, 20,  2],\n",
      "        [ 1, 16, 22,  5, 21, 22, 16,  2],\n",
      "        [ 1, 26,  6, 23, 21, 22, 16,  2],\n",
      "        [ 1,  5, 22, 16, 21, 22, 16,  2],\n",
      "        [ 1,  5, 14, 16, 10, 19, 22,  2]], device='cuda:0'), batch_size: 16, word_len: 8\n",
      "13: batch_data: tensor([[ 1, 29, 22, 19, 24, 20, 21, 22, 16,  2],\n",
      "        [ 1,  9, 22, 27, 24, 11, 26, 22, 16,  2],\n",
      "        [ 1, 15, 22, 28, 24, 23, 12, 22, 16,  2],\n",
      "        [ 1, 28, 14, 13, 19, 10, 30, 22, 23,  2],\n",
      "        [ 1, 12,  4, 20, 16,  6, 26,  6,  7,  2],\n",
      "        [ 1,  4,  5,  4, 12,  6, 19,  6, 16,  2],\n",
      "        [ 1, 25, 14, 27, 10, 11, 21, 14, 23,  2],\n",
      "        [ 1, 31, 14, 29, 10, 23, 21, 14, 16,  2],\n",
      "        [ 1, 16,  6, 11, 22,  9, 10, 15, 14,  2],\n",
      "        [ 1, 14,  5,  5, 10, 11, 21, 14, 16,  2],\n",
      "        [ 1, 16, 22, 28, 22, 26, 10,  5, 14,  2],\n",
      "        [ 1, 25, 10, 11, 10, 19, 21, 14, 16,  2]], device='cuda:0'), batch_size: 12, word_len: 10\n"
     ]
    }
   ],
   "source": [
    "## Train\n",
    "epochs = 50\n",
    "for epoch in range(epochs):\n",
    "    loss = 0\n",
    "    epoch_acc = 0\n",
    "    epoch_tokens = 0  \n",
    "    for idx,i in enumerate(np.random.permutation(len(train_data_batch))):\n",
    "        batch_data = train_data_batch[i]\n",
    "        batch_size, word_len = batch_data.size()\n",
    "        # calculate model loss with batch_data and update gradients\n",
    "        print(f\"{idx}: batch_data: {batch_data}, batch_size: {batch_size}, word_len: {word_len}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "transformers-startup-code.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
